{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Maintenance\n",
    "\n",
    "## Step 1: Preprocessing\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook you need to install:\n",
    "\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `sklearn`\n",
    "\n",
    "The easiest way is to install these libraries with `pip`, which is the python package installation tool.\n",
    "You can simply use\n",
    "\n",
    "    pip install pandas\n",
    "    pip install numpy\n",
    "    pip install sklearn\n",
    "\n",
    "which should install everything easily.\n",
    "\n",
    "### What does this file do ?\n",
    "\n",
    "This file constitues the first step of the predicitive maintenance process for the NASA turbines dataset.\n",
    "\n",
    "### When do I need to run it ?\n",
    "\n",
    "This file extracts the raw data from the `..\\input\\` folder and preprocess it in order to create useable data for the rest of the process.\n",
    "\n",
    "Thus, this script is necessary each time raw data is changed (new train set, new test set or new real-world-based test failure observation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion\n",
    "\n",
    "In this step, data is extracted from txt files.\n",
    "\n",
    "This dataset contains records of NASA turbines. The train set holds the engine run-to-failure data. The test set holds the engine operating data without failure events recorded. Finally, the truth set contains the information of true remaining cycles for each engine in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3','s4', 's5', 's6', 's7', 's8',\n",
    "         's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "# read training data\n",
    "train_data = pd.read_csv('input/TrainSet.txt', sep=\" \", header=None)\n",
    "train_data.drop(train_data.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_data.columns = names\n",
    "\n",
    "train_data = train_data.sort_values(['id','cycle'])\n",
    "\n",
    "# read test data\n",
    "test_data = pd.read_csv('input/TestSet.txt', sep=\" \", header=None)\n",
    "test_data.drop(test_data.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_data.columns = names\n",
    "\n",
    "# read ground truth data\n",
    "truth_df = pd.read_csv('input/TestSet_RUL.txt', sep=\" \", header=None)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the size of the train dataset: 20631 entries and 26 features\n",
      "This is the size of the test dataset: 13096 entries and 26 features\n",
      "This is the size of the truth dataset: 100 entries and 1 features\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the size of the train dataset: {} entries and {} features\".format(train_data.shape[0], \n",
    "                                                                                 train_data.shape[1]))\n",
    "print(\"This is the size of the test dataset: {} entries and {} features\".format(test_data.shape[0],\n",
    "                                                                                test_data.shape[1]))\n",
    "print(\"This is the size of the truth dataset: {} entries and {} features\".format(truth_df.shape[0],\n",
    "                                                                                 truth_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 100 turbines in each dataset\n"
     ]
    }
   ],
   "source": [
    "n_turb = train_data[\"id\"].unique().max()\n",
    "n_train, n_features = train_data.shape\n",
    "print(\"There is {} turbines in each dataset\".format(n_turb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "This step adds new features to train and test set, which will constitutes the labels for the coming prediction algorithms.\n",
    "\n",
    "### 2.1 Train Set\n",
    "\n",
    "For this train set, we calculate the Remaining Useful Life (RUL) for each cycle of each turbine.\n",
    "\n",
    "Then, we generate labels for a hypothetical binary classification, while trying to answer the question: is a specific engine going to fail within n cycles ? These labels aren't used in the following learning algorithms, but could be useful for a (future ?) binary classification step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Labeling - generate column RUL\n",
    "rul = pd.DataFrame(train_data.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "train_data = train_data.merge(rul, on=['id'], how='left')\n",
    "train_data['RUL'] = train_data['max'] - train_data['cycle']\n",
    "train_data.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# generate label columns\n",
    "w1 = 30\n",
    "w0 = 15\n",
    "train_data['label1'] = np.where(train_data['RUL'] <= w1, 1, 0 )\n",
    "train_data['label2'] = train_data['label1']\n",
    "train_data.loc[train_data['RUL'] <= w0, 'label2'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the values of the different features are widely scattered, it is interesting to normalize them. Here, we use the min-max normalisation to perform it.\n",
    "\n",
    "Only the settings and the parameters are normalized (in place), as well as the cycle's number (in an other column). The other variables are left untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train data set is now: 20631 entries and 30 features.\n",
      "Train Data saved as input/train.csv\n"
     ]
    }
   ],
   "source": [
    "# MinMax normalization (from 0 to 1)\n",
    "train_data['cycle_norm'] = train_data['cycle']\n",
    "cols_normalize = train_data.columns.difference(['id','cycle','RUL','label1','label2'])\n",
    "min_max_scaler = MinMaxScaler()\n",
    "norm_train_data = pd.DataFrame(min_max_scaler.fit_transform(train_data[cols_normalize]),\n",
    "                               columns=cols_normalize, index=train_data.index)\n",
    "join_data = train_data[train_data.columns.difference(cols_normalize)].join(norm_train_data)\n",
    "train_data = join_data.reindex(columns = train_data.columns)\n",
    "\n",
    "print(\"The size of the train data set is now: {} entries and {} features.\".format(train_data.shape[0],\n",
    "                                                                                  train_data.shape[1]))\n",
    "\n",
    "train_data.to_csv('input/train.csv', encoding='utf-8',index = None)\n",
    "print(\"Train Data saved as input/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Test Set\n",
    "\n",
    "The process is similar to the train set one.\n",
    "\n",
    "However, the RUL is calculated based on the values in the truth data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the test data set is now: 13096 entries and 30 features.\n",
      "Test Data saved as input/test.csv\n"
     ]
    }
   ],
   "source": [
    "# MinMax normalization (from 0 to 1)\n",
    "test_data['cycle_norm'] = test_data['cycle']\n",
    "norm_test_data = pd.DataFrame(min_max_scaler.transform(test_data[cols_normalize]),\n",
    "                              columns=cols_normalize, index=test_data.index)\n",
    "test_join_data = test_data[test_data.columns.difference(cols_normalize)].join(norm_test_data)\n",
    "test_data = test_join_data.reindex(columns = test_data.columns)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# generate RUL\n",
    "rul = pd.DataFrame(test_data.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "test_data = test_data.merge(truth_df, on=['id'], how='left')\n",
    "test_data['RUL'] = test_data['max'] - test_data['cycle']\n",
    "test_data.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# generate label columns w0 and w1 for test data\n",
    "test_data['label1'] = np.where(test_data['RUL'] <= w1, 1, 0 )\n",
    "test_data['label2'] = test_data['label1']\n",
    "test_data.loc[test_data['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "print(\"The size of the test data set is now: {} entries and {} features.\".format(test_data.shape[0],\n",
    "                                                                                 test_data.shape[1]))\n",
    "\n",
    "test_data.to_csv('input/test.csv', encoding='utf-8',index = None)\n",
    "print(\"Test Data saved as input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
